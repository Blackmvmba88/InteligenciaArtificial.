# MambaCore v2 - Sinapsis Multimodal

## VisiÃ³n General

MambaCore v2 extiende el framework base de InteligenciaArtificial con capacidades multimodales, permitiendo que el sistema cognitivo:

- ğŸ¨ **Vea y cree**: Genere y analice imÃ¡genes
- ğŸµ **Escuche y componga**: Genere mÃºsica y sintetice voz
- ğŸ’¬ **Comprenda y hable**: Procese lenguaje natural
- ğŸ”— **Integre todo**: Sincronice percepciones multimodales

## Arquitectura Multimodal

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MambaCLI (Usuario)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ Vision   â”‚          â”‚  Audio   â”‚
    â”‚ Module   â”‚          â”‚  Module  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
         â”‚  Synapse    â”‚
         â”‚   Bridge    â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
         â”‚  EventBus   â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
         â”‚  Cognitive  â”‚
         â”‚    Core     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Componentes

### 1. VisionModule

Interfaz para generaciÃ³n y anÃ¡lisis de imÃ¡genes.

**Backends soportados:**
- `mock` - SimulaciÃ³n sin dependencias (por defecto)
- `stable_diffusion` - Stable Diffusion WebUI API
- `comfyui` - ComfyUI API

**Ejemplo de uso:**

```python
from src.multimodal.vision_module import VisionModule

vision = VisionModule("vision", event_bus, backend="mock")

# Generar imagen
await vision.generate("Paisaje cyberpunk con neÃ³n y lluvia")

# Analizar imagen
result = await vision.analyze_image("path/to/image.jpg")
```

**IntegraciÃ³n con Stable Diffusion:**

```python
# TODO: Configurar SD WebUI con --api flag
# vision = VisionModule("vision", event_bus, backend="stable_diffusion")
# Requiere: requests, pillow (opcional)
```

### 2. AudioModule

Interfaz para generaciÃ³n de audio, mÃºsica y sÃ­ntesis de voz.

**Backends soportados:**
- `mock` - SimulaciÃ³n sin dependencias (por defecto)
- `suno` - Suno AI API
- `tts_local` - TTS local (pyttsx3, espeak)
- `music_gen` - MusicGen / AudioCraft

**Ejemplo de uso:**

```python
from src.multimodal.audio_module import AudioModule

audio = AudioModule("audio", event_bus, backend="mock")

# Generar mÃºsica
await audio.generate_music("Jazz suave con piano", style="ambient")

# SÃ­ntesis de voz
await audio.generate_speech("Hola mundo", voice="default")

# Efecto de sonido
await audio.generate_sound("Lluvia cayendo")
```

**IntegraciÃ³n con Suno:**

```python
# TODO: Configurar API key de Suno
# audio = AudioModule("audio", event_bus, backend="suno")
# Requiere: suno-client (pip install suno-client)
```

### 3. CommandInterface (MambaCLI)

Terminal inteligente que interpreta comandos en lenguaje natural.

**Comandos:**
- `vision <descripciÃ³n>` - Generar imagen
- `audio <descripciÃ³n>` - Generar mÃºsica
- `speech <texto>` - SÃ­ntesis de voz
- `status` - Estado del sistema
- `history` - Historial de comandos
- `help` - Ayuda
- `exit` - Salir

**Ejemplo de uso:**

```python
from src.multimodal.command_interface import CommandInterface

cli = CommandInterface("mamba", event_bus, verbose=True)

# Ejecutar comando
await cli.execute_command("genera una imagen de robot futurista")

# Modo interactivo
await cli.start_interactive_mode()
```

### 4. SynapseBridge

Puente sinÃ¡ptico que sincroniza contexto entre modalidades.

**CaracterÃ­sticas:**
- Mantiene contexto compartido entre visiÃ³n, audio y texto
- Prepara embeddings unificados (estilo CLIP)
- BÃºsqueda cross-modal
- Historial de sincronizaciones

**Ejemplo de uso:**

```python
from src.multimodal.synapse_bridge import SynapseBridge

bridge = SynapseBridge(event_bus, context_window=50)

# Obtener contexto unificado
unified = bridge.get_unified_context(limit=10)

# Contexto por modalidad
vision_context = bridge.get_context_by_modality("vision", limit=5)
audio_context = bridge.get_context_by_modality("audio", limit=5)

# BÃºsqueda cross-modal
results = await bridge.cross_modal_search("paisaje", modality="all")

# EstadÃ­sticas
stats = bridge.get_statistics()
```

## Inicio RÃ¡pido

### 1. Demo AutomÃ¡tico

```bash
python3 examples_mambacore_v2.py
```

Ejecuta ejemplos automÃ¡ticos de:
- GeneraciÃ³n de imÃ¡genes
- GeneraciÃ³n de mÃºsica
- SÃ­ntesis de voz
- AnÃ¡lisis multimodal

### 2. Modo Interactivo

```bash
python3 examples_mambacore_v2_interactive.py
```

InteractÃºa en tiempo real:
```
mamba> vision crea un dragÃ³n de cristal volando
[MambaCore] ğŸ¨ Generando imagen: crea un dragÃ³n de cristal volando

mamba> audio mÃºsica electrÃ³nica atmosfÃ©rica
[MambaCore] ğŸµ Generando audio: mÃºsica electrÃ³nica atmosfÃ©rica

mamba> status
[MambaCLI] ğŸ“Š Estado del sistema:
  Acciones ejecutadas: 2
  Comandos en historial: 2
```

### 3. Crear tu Propio Sistema Multimodal

```python
import asyncio
from src.core.event_bus import EventBus
from src.core.cognitive_core import CognitiveCore
from src.modules.memory_module import MemoryModule
from src.modules.reasoning_engine import ReasoningEngine
from src.multimodal import VisionModule, AudioModule, CommandInterface, SynapseBridge

async def main():
    # Inicializar componentes
    bus = EventBus()
    memory = MemoryModule("my_mamba.json")
    reasoning = ReasoningEngine()
    
    # Crear nÃºcleo cognitivo
    core = CognitiveCore(bus, memory, reasoning)
    
    # AÃ±adir capacidades multimodales
    vision = VisionModule("vision", bus, backend="mock")
    audio = AudioModule("audio", bus, backend="mock")
    cli = CommandInterface("cli", bus)
    bridge = SynapseBridge(bus)
    
    # Ejecutar
    await asyncio.gather(
        bus.start(),
        core.run(cycle_interval=1.0),
        vision.start(interval=0.5),
        audio.start(interval=0.5)
    )

if __name__ == "__main__":
    asyncio.run(main())
```

## Extensiones Futuras

### Cognitive Mesh (Red Cognitiva Distribuida)

```python
# TODO: Implementar comunicaciÃ³n distribuida
# Raspberry Pi â†’ sensores fÃ­sicos
# Mac â†’ nÃºcleo cognitivo y render
# Android (Termux) â†’ CLI Mamba remoto
# ComunicaciÃ³n: WebSockets + ZeroMQ + SQLite replicado
```

### QuantumBridge (Motor de Resonancia Creativa)

```python
# TODO: Implementar motor cuÃ¡ntico de creatividad
# - Patrones no deterministas
# - Interferencia de decisiones
# - "Estado emocional" del sistema
# - Rutas creativas evolutivas
```

### Memoria Vectorial SemÃ¡ntica

```python
# TODO: Implementar almacenamiento vectorial
# - IntegraciÃ³n con FAISS o Qdrant
# - Embeddings con CLIP/CLAP/GPT
# - BÃºsqueda semÃ¡ntica cross-modal
# - "SueÃ±os": combinaciones creativas de percepciones
```

### Panel Web Visual

```python
# TODO: Implementar visualizaciÃ³n web
# - FastAPI + React + WebSocket
# - Flujo cognitivo en tiempo real
# - VisualizaciÃ³n de percepciones y decisiones
# - Control remoto del sistema
```

## IntegraciÃ³n con Herramientas Externas

### Stable Diffusion

1. Instalar Stable Diffusion WebUI con API
2. Configurar backend:
   ```python
   vision = VisionModule("vision", bus, backend="stable_diffusion")
   ```
3. Opcional: Instalar dependencias
   ```bash
   pip install requests pillow
   ```

### Suno (MÃºsica)

1. Obtener API key de Suno
2. Instalar cliente:
   ```bash
   pip install suno-client
   ```
3. Configurar backend:
   ```python
   audio = AudioModule("audio", bus, backend="suno")
   ```

### ComfyUI

1. Instalar ComfyUI con API habilitada
2. Configurar backend:
   ```python
   vision = VisionModule("vision", bus, backend="comfyui")
   ```

### TTS Local

1. Instalar pyttsx3:
   ```bash
   pip install pyttsx3
   ```
2. Configurar backend:
   ```python
   audio = AudioModule("audio", bus, backend="tts_local")
   ```

## Testing

Ejecutar tests multimodales:

```bash
python3 test_mambacore_v2.py
```

Tests incluidos:
1. VisionModule - GeneraciÃ³n de imÃ¡genes
2. AudioModule - GeneraciÃ³n de audio
3. CommandInterface - CLI Mamba
4. SynapseBridge - SincronizaciÃ³n multimodal
5. IntegraciÃ³n - Sistema completo

## FilosofÃ­a de DiseÃ±o

1. **Ligero por defecto**: Funciona sin dependencias externas (backends mock)
2. **Extensible**: FÃ¡cil integraciÃ³n con herramientas reales
3. **Modular**: Cada componente independiente
4. **Evolutivo**: Preparado para crecimiento futuro
5. **Portable**: Compatible con mÃºltiples plataformas

## Contribuir

Para aÃ±adir nuevos backends:

1. Crear mÃ©todo `_<backend>_generate` en el mÃ³dulo
2. AÃ±adir backend al enum de opciones
3. Documentar dependencias necesarias
4. Crear test para el backend

Ejemplo:
```python
async def _my_backend_generate(self, prompt: str) -> Dict[str, Any]:
    # Tu implementaciÃ³n
    return {
        "type": "vision",
        "prompt": prompt,
        "image": result,
        "backend": "my_backend"
    }
```

## Recursos

- [README.md](README.md) - DocumentaciÃ³n principal
- [QUICKSTART.md](QUICKSTART.md) - GuÃ­a rÃ¡pida
- [examples_mambacore_v2.py](examples_mambacore_v2.py) - Demo completo
- [test_mambacore_v2.py](test_mambacore_v2.py) - Suite de tests

---

**MambaCore v2 - Donde visiÃ³n, audio y lenguaje se encuentran en una sola conciencia cognitiva. ğŸ§¬**
